{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:40:39.659648: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-11 18:40:46.027230: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-11 18:40:46.035543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 18:40:52.805490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 1,64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize dataset data into range of tanh (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real_train = x_real_train.astype('float32')\n",
    "x_real_train /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(50,2,size=(1,100))\n",
    "    input = input * 10\n",
    "    \n",
    "\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8,8,60)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(8,8,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(16,16,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (5,5), strides=(2,2), padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(1024, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "   \n",
    "    model.add(Conv2D(1024, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(2048, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3840)              387840    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 3840)              15360     \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 1)         60        \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 1)         4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 1)         1         \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 1)         4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 1)         1         \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403270 (1.54 MB)\n",
      "Trainable params: 395586 (1.51 MB)\n",
      "Non-trainable params: 7684 (30.02 KB)\n",
      "_________________________________________________________________\n",
      "tf.Tensor(\n",
      "[[[[4.6757196e-19]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]]], shape=(1, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "test_gen = gen_model(noise, training = False)\n",
    "\n",
    "print(test_gen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 256)       6656      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 256)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 1024)        6554624   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 1024)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 1024)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 1024)        26215424  \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 2, 2, 1024)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 1024)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 2048)        52430848  \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 1, 2048)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85209601 (325.05 MB)\n",
      "Trainable params: 85209601 (325.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[0.00061947]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(test_gen)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(learning_rate =1e-4, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate =1e-4, beta_1=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrim_accuracy(real_output, fake_output):\n",
    "    if real_output >=0:\n",
    "        \n",
    "        if fake_output <0:\n",
    "            accuracy = (real_output + fake_output) / (real_output + fake_output)\n",
    "        else:\n",
    "            accuracy = real_output/ (real_output + fake_output)\n",
    "    elif fake_output <0:\n",
    "        accuracy = fake_output / (real_output + fake_output)\n",
    "    else:\n",
    "        accuracy = 0/ (real_output + fake_output)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "        print(\"generated map shape\" + str(generated_map.shape))\n",
    "        \n",
    "        fake_output = disc_model(generated_map,  training=True)\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        \n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        disc_accuracy = compute_discrim_accuracy(real_output, fake_output)\n",
    "\n",
    "        \n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, disc_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    disc_accuracies = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        gen_losses_for_epoch = []\n",
    "        disc_losses_for_epoch = []\n",
    "\n",
    "        print(\"epoch = \" + str(epoch))\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            \n",
    "            map_batch.reshape(1,64,64,1)\n",
    "\n",
    "            \n",
    "            gen_loss, disc_loss, disc_accuracy = training_step(map_batch)\n",
    "\n",
    "            gen_losses_for_epoch.append(gen_loss)\n",
    "            disc_losses_for_epoch.append(disc_loss)\n",
    "            disc_accuracies.append(disc_accuracy)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses_for_epoch) / 48\n",
    "        avg_disc_loss = sum(disc_losses_for_epoch) / 48\n",
    "\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "\n",
    "        print(\"Gen loss = \" + str(avg_gen_loss))\n",
    "        print(\"Disc loss = \" + str(avg_disc_loss))\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    generated_map = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    \n",
    "\n",
    "    return gen_losses, disc_losses, generated_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 64, 64, 1)\n",
      "epoch = 0\n",
      "generated map shape(1, 64, 64, 1)\n",
      "generated map shape(1, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:41:24.743228: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_BOOL\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond/PartitionedCall/cond_3/output/_20'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss = tf.Tensor(1.148497, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.8528146, shape=(), dtype=float32)\n",
      "epoch = 1\n",
      "Gen loss = tf.Tensor(1.3130428, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62688255, shape=(), dtype=float32)\n",
      "epoch = 2\n",
      "Gen loss = tf.Tensor(1.3130723, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266746, shape=(), dtype=float32)\n",
      "epoch = 3\n",
      "Gen loss = tf.Tensor(1.3131303, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266269, shape=(), dtype=float32)\n",
      "epoch = 4\n",
      "Gen loss = tf.Tensor(1.3131654, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266286, shape=(), dtype=float32)\n",
      "epoch = 5\n",
      "Gen loss = tf.Tensor(1.3131747, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62658155, shape=(), dtype=float32)\n",
      "epoch = 6\n",
      "Gen loss = tf.Tensor(1.3132111, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265617, shape=(), dtype=float32)\n",
      "epoch = 7\n",
      "Gen loss = tf.Tensor(1.3132194, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626549, shape=(), dtype=float32)\n",
      "epoch = 8\n",
      "Gen loss = tf.Tensor(1.3132268, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265529, shape=(), dtype=float32)\n",
      "epoch = 9\n",
      "Gen loss = tf.Tensor(1.3132304, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265436, shape=(), dtype=float32)\n",
      "epoch = 10\n",
      "Gen loss = tf.Tensor(1.313239, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653846, shape=(), dtype=float32)\n",
      "epoch = 11\n",
      "Gen loss = tf.Tensor(1.3132428, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626543, shape=(), dtype=float32)\n",
      "epoch = 12\n",
      "Gen loss = tf.Tensor(1.3132453, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265353, shape=(), dtype=float32)\n",
      "epoch = 13\n",
      "Gen loss = tf.Tensor(1.3132471, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265332, shape=(), dtype=float32)\n",
      "epoch = 14\n",
      "Gen loss = tf.Tensor(1.3132473, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265325, shape=(), dtype=float32)\n",
      "epoch = 15\n",
      "Gen loss = tf.Tensor(1.3132501, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653404, shape=(), dtype=float32)\n",
      "epoch = 16\n",
      "Gen loss = tf.Tensor(1.3132507, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265314, shape=(), dtype=float32)\n",
      "epoch = 17\n",
      "Gen loss = tf.Tensor(1.3132526, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653035, shape=(), dtype=float32)\n",
      "epoch = 18\n",
      "Gen loss = tf.Tensor(1.313254, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265276, shape=(), dtype=float32)\n",
      "epoch = 19\n",
      "Gen loss = tf.Tensor(1.3132552, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265282, shape=(), dtype=float32)\n",
      "epoch = 20\n",
      "Gen loss = tf.Tensor(1.3132553, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652737, shape=(), dtype=float32)\n",
      "epoch = 21\n",
      "Gen loss = tf.Tensor(1.3132571, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265277, shape=(), dtype=float32)\n",
      "epoch = 22\n",
      "Gen loss = tf.Tensor(1.3132558, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626527, shape=(), dtype=float32)\n",
      "epoch = 23\n",
      "Gen loss = tf.Tensor(1.3132571, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265266, shape=(), dtype=float32)\n",
      "epoch = 24\n",
      "Gen loss = tf.Tensor(1.3132576, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652606, shape=(), dtype=float32)\n",
      "epoch = 25\n",
      "Gen loss = tf.Tensor(1.3132573, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265271, shape=(), dtype=float32)\n",
      "epoch = 26\n",
      "Gen loss = tf.Tensor(1.3132577, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265267, shape=(), dtype=float32)\n",
      "epoch = 27\n",
      "Gen loss = tf.Tensor(1.313258, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265265, shape=(), dtype=float32)\n",
      "epoch = 28\n",
      "Gen loss = tf.Tensor(1.3132585, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265263, shape=(), dtype=float32)\n",
      "epoch = 29\n",
      "Gen loss = tf.Tensor(1.3132583, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265262, shape=(), dtype=float32)\n",
      "epoch = 30\n",
      "Gen loss = tf.Tensor(1.3132583, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652653, shape=(), dtype=float32)\n",
      "epoch = 31\n",
      "Gen loss = tf.Tensor(1.3132583, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265256, shape=(), dtype=float32)\n",
      "epoch = 32\n",
      "Gen loss = tf.Tensor(1.3132588, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652564, shape=(), dtype=float32)\n",
      "epoch = 33\n",
      "Gen loss = tf.Tensor(1.3132586, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626525, shape=(), dtype=float32)\n",
      "epoch = 34\n",
      "Gen loss = tf.Tensor(1.3132591, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652516, shape=(), dtype=float32)\n",
      "epoch = 35\n",
      "Gen loss = tf.Tensor(1.3132595, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265248, shape=(), dtype=float32)\n",
      "epoch = 36\n",
      "Gen loss = tf.Tensor(1.3132596, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626525, shape=(), dtype=float32)\n",
      "epoch = 37\n",
      "Gen loss = tf.Tensor(1.3132592, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652475, shape=(), dtype=float32)\n",
      "epoch = 38\n",
      "Gen loss = tf.Tensor(1.3132595, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265246, shape=(), dtype=float32)\n",
      "epoch = 39\n",
      "Gen loss = tf.Tensor(1.3132596, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265245, shape=(), dtype=float32)\n",
      "epoch = 40\n",
      "Gen loss = tf.Tensor(1.3132601, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265246, shape=(), dtype=float32)\n",
      "epoch = 41\n",
      "Gen loss = tf.Tensor(1.31326, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265251, shape=(), dtype=float32)\n",
      "epoch = 42\n",
      "Gen loss = tf.Tensor(1.3132597, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652475, shape=(), dtype=float32)\n",
      "epoch = 43\n",
      "Gen loss = tf.Tensor(1.31326, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265243, shape=(), dtype=float32)\n",
      "epoch = 44\n",
      "Gen loss = tf.Tensor(1.31326, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265247, shape=(), dtype=float32)\n",
      "epoch = 45\n",
      "Gen loss = tf.Tensor(1.3132606, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 46\n",
      "Gen loss = tf.Tensor(1.3132607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265242, shape=(), dtype=float32)\n",
      "epoch = 47\n",
      "Gen loss = tf.Tensor(1.3132607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265244, shape=(), dtype=float32)\n",
      "epoch = 48\n",
      "Gen loss = tf.Tensor(1.3132602, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265246, shape=(), dtype=float32)\n",
      "epoch = 49\n",
      "Gen loss = tf.Tensor(1.3132607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265244, shape=(), dtype=float32)\n",
      "epoch = 50\n",
      "Gen loss = tf.Tensor(1.3132607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265245, shape=(), dtype=float32)\n",
      "epoch = 51\n",
      "Gen loss = tf.Tensor(1.3132603, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265241, shape=(), dtype=float32)\n",
      "epoch = 52\n",
      "Gen loss = tf.Tensor(1.3132607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 53\n",
      "Gen loss = tf.Tensor(1.3132609, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652427, shape=(), dtype=float32)\n",
      "epoch = 54\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 55\n",
      "Gen loss = tf.Tensor(1.3132613, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 56\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626524, shape=(), dtype=float32)\n",
      "epoch = 57\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 58\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626524, shape=(), dtype=float32)\n",
      "epoch = 59\n",
      "Gen loss = tf.Tensor(1.3132613, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265239, shape=(), dtype=float32)\n",
      "epoch = 60\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265241, shape=(), dtype=float32)\n",
      "epoch = 61\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265244, shape=(), dtype=float32)\n",
      "epoch = 62\n",
      "Gen loss = tf.Tensor(1.3132615, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 63\n",
      "Gen loss = tf.Tensor(1.3132614, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 64\n",
      "Gen loss = tf.Tensor(1.3132612, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652385, shape=(), dtype=float32)\n",
      "epoch = 65\n",
      "Gen loss = tf.Tensor(1.3132615, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265239, shape=(), dtype=float32)\n",
      "epoch = 66\n",
      "Gen loss = tf.Tensor(1.3132614, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626524, shape=(), dtype=float32)\n",
      "epoch = 67\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652385, shape=(), dtype=float32)\n",
      "epoch = 68\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 69\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 70\n",
      "Gen loss = tf.Tensor(1.3132615, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 71\n",
      "Gen loss = tf.Tensor(1.3132615, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 72\n",
      "Gen loss = tf.Tensor(1.3132614, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 73\n",
      "Gen loss = tf.Tensor(1.3132617, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 74\n",
      "Gen loss = tf.Tensor(1.3132617, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652415, shape=(), dtype=float32)\n",
      "epoch = 75\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652385, shape=(), dtype=float32)\n",
      "epoch = 76\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 77\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652385, shape=(), dtype=float32)\n",
      "epoch = 78\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 79\n",
      "Gen loss = tf.Tensor(1.313262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 80\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652403, shape=(), dtype=float32)\n",
      "epoch = 81\n",
      "Gen loss = tf.Tensor(1.313262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265239, shape=(), dtype=float32)\n",
      "epoch = 82\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 83\n",
      "Gen loss = tf.Tensor(1.3132617, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 84\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 85\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 86\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "epoch = 87\n",
      "Gen loss = tf.Tensor(1.3132616, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 88\n",
      "Gen loss = tf.Tensor(1.3132619, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 89\n",
      "Gen loss = tf.Tensor(1.313262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 90\n",
      "Gen loss = tf.Tensor(1.313262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "epoch = 91\n",
      "Gen loss = tf.Tensor(1.313262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 92\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "epoch = 93\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265236, shape=(), dtype=float32)\n",
      "epoch = 94\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "epoch = 95\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 96\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652373, shape=(), dtype=float32)\n",
      "epoch = 97\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "epoch = 98\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265238, shape=(), dtype=float32)\n",
      "epoch = 99\n",
      "Gen loss = tf.Tensor(1.3132621, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265237, shape=(), dtype=float32)\n",
      "(64, 64)\n",
      "[[0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 255.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 0.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0, 107.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0], [121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0, 121.0, 128.0], [128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0, 128.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses, generated_map = train(x_real_train, N_EPOCHS)\n",
    "\n",
    "# denormalise generated map\n",
    "\n",
    "generated_map *= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generated_map = generated_map.numpy()\n",
    "\n",
    "generated_map = np.round(generated_map,0)\n",
    "\n",
    "generated_map = np.reshape(generated_map, (64,64))\n",
    "\n",
    "print(generated_map.shape)\n",
    "\n",
    "#write generated map to csv\n",
    "\n",
    "\n",
    "np.savetxt('generated_map.csv', generated_map, delimiter=',')\n",
    "\n",
    "generated_map = generated_map.tolist()\n",
    "\n",
    "print(generated_map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = [i for i in range(0,N_EPOCHS)]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epoch_list,disc_losses,  'r-')\n",
    "plt.plot(epoch_list, gen_losses, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
