{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(3,2.5,size=(1,100))\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Reshape((8,8,60)))\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(8,8,60)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(16,16,60)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, kernel_size=5, padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 3840)              387840    \n",
      "                                                                 \n",
      " activation_96 (Activation)  (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape_18 (Reshape)        (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_55 (Conv2  (None, 16, 16, 1)         61        \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_97 (Activation)  (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_56 (Conv2  (None, 32, 32, 1)         2         \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_98 (Activation)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_57 (Conv2  (None, 64, 64, 1)         2         \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_99 (Activation)  (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387905 (1.48 MB)\n",
      "Trainable params: 387905 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "generated_map = gen_model(noise, training = False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 128)       3328      \n",
      "                                                                 \n",
      " activation_100 (Activation  (None, 64, 64, 128)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 32, 32, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 1)                 131073    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134401 (525.00 KB)\n",
      "Trainable params: 134401 (525.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(generated_map)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(np.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(np.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(np.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(1e-4)\n",
    "disc_optimizer = Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 48\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        fake_output = disc_model(generated_map, training = True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            print(map_batch.shape)\n",
    "            training_step(map_batch)\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    map_after_training = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    print(map_after_training)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 64, 64, 1)\n",
      "(64, 64, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_102226/2522044098.py\", line 10, in training_step  *\n        real_output = disc_model(current_batch, training=True, verbose=VERBOSE)\n    File \"/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Sequential.call() got an unexpected keyword argument 'verbose'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(x_real_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m train(x_real_train, N_EPOCHS)\n",
      "Cell \u001b[0;32mIn[121], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, N_EPOCHS)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m map_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m      5\u001b[0m         \u001b[39mprint\u001b[39m(map_batch\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m         training_step(map_batch)\n\u001b[1;32m      8\u001b[0m input_for_map_after_training \u001b[39m=\u001b[39m generate_generator_input()\n\u001b[1;32m      9\u001b[0m map_after_training \u001b[39m=\u001b[39m gen_model(input_for_map_after_training, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6blg9nmo.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[0;34m(current_batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m gen_tape, ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m disc_tape:\n\u001b[1;32m     10\u001b[0m     generated_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(gen_model), (ag__\u001b[39m.\u001b[39mld(noise_sample),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m---> 11\u001b[0m     real_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(disc_model), (ag__\u001b[39m.\u001b[39;49mld(current_batch),), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(VERBOSE)), fscope)\n\u001b[1;32m     12\u001b[0m     fake_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(disc_model), (ag__\u001b[39m.\u001b[39mld(generated_map),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     13\u001b[0m     gen_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(generator_loss), (), \u001b[39mdict\u001b[39m(fake_output\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(fake_output)), fscope)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_102226/2522044098.py\", line 10, in training_step  *\n        real_output = disc_model(current_batch, training=True, verbose=VERBOSE)\n    File \"/home/ubuntu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Sequential.call() got an unexpected keyword argument 'verbose'\n"
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "train(x_real_train, N_EPOCHS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
