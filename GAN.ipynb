{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:44:13.432214: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 13:44:15.825179: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 13:44:15.834819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 13:44:20.767492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 1,64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize dataset data into range of tanh (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real_train = x_real_train.astype('float32')\n",
    "x_real_train /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(50,2,size=(1,100))\n",
    "    input = input * 10\n",
    "    \n",
    "\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8,8,60)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, (1,1), strides=(2,2), padding='same', input_shape=(8,8,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(64, (1,1), strides=(2,2), padding='same', input_shape=(16,16,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (5,5), strides=(2,2), padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "   \n",
    "    model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3840)              387840    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 3840)              15360     \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 128)       7808      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 64)        8256      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 1)         65        \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420097 (1.60 MB)\n",
      "Trainable params: 412033 (1.57 MB)\n",
      "Non-trainable params: 8064 (31.50 KB)\n",
      "_________________________________________________________________\n",
      "tf.Tensor(\n",
      "[[[[0.99636513]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]\n",
      "\n",
      "  [[0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   ...\n",
      "   [0.        ]\n",
      "   [0.        ]\n",
      "   [0.        ]]]], shape=(1, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "test_gen = gen_model(noise, training = False)\n",
    "\n",
    "print(test_gen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 256)       6656      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 256)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 512)         3277312   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 512)         6554112   \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9838593 (37.53 MB)\n",
      "Trainable params: 9838593 (37.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:44:32.658421: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26214400 exceeds 10% of free system memory.\n",
      "2023-08-10 13:44:32.707569: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26214400 exceeds 10% of free system memory.\n",
      "2023-08-10 13:44:32.735507: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26214400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.02260379]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(test_gen)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(1e-4)\n",
    "disc_optimizer = Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrim_accuracy(real_output, fake_output):\n",
    "    if real_output >=0:\n",
    "        \n",
    "        if fake_output <0:\n",
    "            accuracy = (real_output + fake_output) / (real_output + fake_output)\n",
    "        else:\n",
    "            accuracy = real_output/ (real_output + fake_output)\n",
    "    elif fake_output <0:\n",
    "        accuracy = fake_output / (real_output + fake_output)\n",
    "    else:\n",
    "        accuracy = 0/ (real_output + fake_output)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 25\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "        print(\"generated map shape\" + str(generated_map.shape))\n",
    "        \n",
    "        fake_output = disc_model(generated_map,  training=True)\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        \n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        disc_accuracy = compute_discrim_accuracy(real_output, fake_output)\n",
    "\n",
    "        \n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, disc_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    disc_accuracies = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        gen_losses_for_epoch = []\n",
    "        disc_losses_for_epoch = []\n",
    "\n",
    "        print(\"epoch = \" + str(epoch))\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            \n",
    "            map_batch.reshape(1,64,64,1)\n",
    "\n",
    "            \n",
    "            gen_loss, disc_loss, disc_accuracy = training_step(map_batch)\n",
    "\n",
    "            gen_losses_for_epoch.append(gen_loss)\n",
    "            disc_losses_for_epoch.append(disc_loss)\n",
    "            disc_accuracies.append(disc_accuracy)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses_for_epoch) / 48\n",
    "        avg_disc_loss = sum(disc_losses_for_epoch) / 48\n",
    "\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "\n",
    "        print(\"Gen loss = \" + str(avg_gen_loss))\n",
    "        print(\"Disc loss = \" + str(avg_disc_loss))\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    generated_map = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    \n",
    "\n",
    "    return gen_losses, disc_losses, generated_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 64, 64, 1)\n",
      "epoch = 0\n",
      "generated map shape(1, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:44:38.437900: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26214400 exceeds 10% of free system memory.\n",
      "2023-08-10 13:44:38.475573: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 26214400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated map shape(1, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:44:42.046766: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_BOOL\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond/PartitionedCall/cond_3/output/_20'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss = tf.Tensor(1.013165, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.8296711, shape=(), dtype=float32)\n",
      "epoch = 1\n",
      "Gen loss = tf.Tensor(1.3112935, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62726825, shape=(), dtype=float32)\n",
      "epoch = 2\n",
      "Gen loss = tf.Tensor(1.3124236, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62684786, shape=(), dtype=float32)\n",
      "epoch = 3\n",
      "Gen loss = tf.Tensor(1.3126572, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626784, shape=(), dtype=float32)\n",
      "epoch = 4\n",
      "Gen loss = tf.Tensor(1.3127664, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6267403, shape=(), dtype=float32)\n",
      "epoch = 5\n",
      "Gen loss = tf.Tensor(1.3128716, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266871, shape=(), dtype=float32)\n",
      "epoch = 6\n",
      "Gen loss = tf.Tensor(1.3129032, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266664, shape=(), dtype=float32)\n",
      "epoch = 7\n",
      "Gen loss = tf.Tensor(1.3130481, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266119, shape=(), dtype=float32)\n",
      "epoch = 8\n",
      "Gen loss = tf.Tensor(1.3130994, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265966, shape=(), dtype=float32)\n",
      "epoch = 9\n",
      "Gen loss = tf.Tensor(1.3131145, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626581, shape=(), dtype=float32)\n",
      "epoch = 10\n",
      "Gen loss = tf.Tensor(1.3131547, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62656856, shape=(), dtype=float32)\n",
      "epoch = 11\n",
      "Gen loss = tf.Tensor(1.3131622, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62657046, shape=(), dtype=float32)\n",
      "epoch = 12\n",
      "Gen loss = tf.Tensor(1.3131696, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265629, shape=(), dtype=float32)\n",
      "epoch = 13\n",
      "Gen loss = tf.Tensor(1.3131937, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62655383, shape=(), dtype=float32)\n",
      "epoch = 14\n",
      "Gen loss = tf.Tensor(1.3132113, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265472, shape=(), dtype=float32)\n",
      "epoch = 15\n",
      "Gen loss = tf.Tensor(1.3132132, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654454, shape=(), dtype=float32)\n",
      "epoch = 16\n",
      "Gen loss = tf.Tensor(1.3132155, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265437, shape=(), dtype=float32)\n",
      "epoch = 17\n",
      "Gen loss = tf.Tensor(1.3132217, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265417, shape=(), dtype=float32)\n",
      "epoch = 18\n",
      "Gen loss = tf.Tensor(1.3132296, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265375, shape=(), dtype=float32)\n",
      "epoch = 19\n",
      "Gen loss = tf.Tensor(1.3132296, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265376, shape=(), dtype=float32)\n",
      "epoch = 20\n",
      "Gen loss = tf.Tensor(1.3132325, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653583, shape=(), dtype=float32)\n",
      "epoch = 21\n",
      "Gen loss = tf.Tensor(1.3132372, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265332, shape=(), dtype=float32)\n",
      "epoch = 22\n",
      "Gen loss = tf.Tensor(1.3132406, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653244, shape=(), dtype=float32)\n",
      "epoch = 23\n",
      "Gen loss = tf.Tensor(1.313243, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265311, shape=(), dtype=float32)\n",
      "epoch = 24\n",
      "Gen loss = tf.Tensor(1.3132433, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265318, shape=(), dtype=float32)\n",
      "(1, 64, 64, 1)\n",
      "[[[[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 4D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(generated_map)\n\u001b[1;32m     24\u001b[0m \u001b[39m#write generated map to csv\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m np\u001b[39m.\u001b[39;49msavetxt(\u001b[39m'\u001b[39;49m\u001b[39mgenerated_map.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, generated_map, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:1555\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[39m# Handle 1-dimensional arrays\u001b[39;00m\n\u001b[1;32m   1554\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1556\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected 1D or 2D array, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mD array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m X\u001b[39m.\u001b[39mndim)\n\u001b[1;32m   1557\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# Common case -- 1d array of numbers\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mnames \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 4D array instead"
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses, generated_map = train(x_real_train, N_EPOCHS)\n",
    "\n",
    "# denormalise generated map\n",
    "\n",
    "generated_map *= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generated_map = generated_map.numpy()\n",
    "\n",
    "generated_map = np.round(generated_map,0)\n",
    "\n",
    "generated_map.reshape(64,64)\n",
    "\n",
    "print(generated_map.shape)\n",
    "\n",
    "generated_map = generated_map.tolist()\n",
    "\n",
    "print(generated_map)\n",
    "\n",
    "#write generated map to csv\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('generated_map.csv', generated_map, delimiter=',')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = [i for i in range(0,N_EPOCHS)]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epoch_list,disc_losses,  'r-')\n",
    "plt.plot(epoch_list, gen_losses, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
