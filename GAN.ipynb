{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 1,64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize dataset data into range of tanh (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real_train = x_real_train.astype('float32')\n",
    "x_real_train /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(50,2,size=(1,100))\n",
    "    input = input * 10\n",
    "    \n",
    "\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8,8,60)))\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(8,8,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(16,16,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 3840)              387840    \n",
      "                                                                 \n",
      " batch_normalization_33 (Ba  (None, 3840)              15360     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_33 (Conv2  (None, 16, 16, 1)         61        \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_34 (Ba  (None, 16, 16, 1)         4         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_34 (Conv2  (None, 32, 32, 1)         2         \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_35 (Ba  (None, 32, 32, 1)         4         \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_57 (LeakyReLU)  (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_35 (Conv2  (None, 64, 64, 1)         2         \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403273 (1.54 MB)\n",
      "Trainable params: 395589 (1.51 MB)\n",
      "Non-trainable params: 7684 (30.02 KB)\n",
      "_________________________________________________________________\n",
      "tf.Tensor(\n",
      "[[[[2.4264057e-20]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]\n",
      "\n",
      "  [[5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   ...\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]\n",
      "   [5.0000000e-01]]]], shape=(1, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "test_gen = gen_model(noise, training = False)\n",
    "\n",
    "print(test_gen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 32, 32, 128)       3328      \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 888321 (3.39 MB)\n",
      "Trainable params: 888321 (3.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[0.04123747]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(test_gen)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(1e-4)\n",
    "disc_optimizer = Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrim_accuracy(real_output, fake_output):\n",
    "    if real_output >=0:\n",
    "        \n",
    "        if fake_output <0:\n",
    "            accuracy = (real_output + fake_output) / (real_output + fake_output)\n",
    "        else:\n",
    "            accuracy = real_output/ (real_output + fake_output)\n",
    "    elif fake_output <0:\n",
    "        accuracy = fake_output / (real_output + fake_output)\n",
    "    else:\n",
    "        accuracy = 0/ (real_output + fake_output)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 750\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "        print(\"generated map shape\" + str(generated_map.shape))\n",
    "        \n",
    "        fake_output = disc_model(generated_map,  training=True)\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        \n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        disc_accuracy = compute_discrim_accuracy(real_output, fake_output)\n",
    "\n",
    "        \n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, disc_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    disc_accuracies = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        gen_losses_for_epoch = []\n",
    "        disc_losses_for_epoch = []\n",
    "\n",
    "        print(\"epoch = \" + str(epoch))\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            \n",
    "            map_batch.reshape(1,64,64,1)\n",
    "\n",
    "            \n",
    "            gen_loss, disc_loss, disc_accuracy = training_step(map_batch)\n",
    "\n",
    "            gen_losses_for_epoch.append(gen_loss)\n",
    "            disc_losses_for_epoch.append(disc_loss)\n",
    "            disc_accuracies.append(disc_accuracy)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses_for_epoch) / 48\n",
    "        avg_disc_loss = sum(disc_losses_for_epoch) / 48\n",
    "\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "\n",
    "        print(\"Gen loss = \" + str(avg_gen_loss))\n",
    "        print(\"Disc loss = \" + str(avg_disc_loss))\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    generated_map = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    \n",
    "\n",
    "    return gen_losses, disc_losses, generated_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 64, 64, 1)\n",
      "epoch = 0\n",
      "generated map shape(1, 64, 64, 1)\n",
      "generated map shape(1, 64, 64, 1)\n",
      "Gen loss = tf.Tensor(1.2480885, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(1.166791, shape=(), dtype=float32)\n",
      "epoch = 1\n",
      "Gen loss = tf.Tensor(2.240356, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.451758, shape=(), dtype=float32)\n",
      "epoch = 2\n",
      "Gen loss = tf.Tensor(4.8004375, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.040053066, shape=(), dtype=float32)\n",
      "epoch = 3\n",
      "Gen loss = tf.Tensor(5.730065, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.010757785, shape=(), dtype=float32)\n",
      "epoch = 4\n",
      "Gen loss = tf.Tensor(6.4766636, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.005917425, shape=(), dtype=float32)\n",
      "epoch = 5\n",
      "Gen loss = tf.Tensor(6.9609685, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0038830221, shape=(), dtype=float32)\n",
      "epoch = 6\n",
      "Gen loss = tf.Tensor(7.307405, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.002887706, shape=(), dtype=float32)\n",
      "epoch = 7\n",
      "Gen loss = tf.Tensor(7.715206, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.002235092, shape=(), dtype=float32)\n",
      "epoch = 8\n",
      "Gen loss = tf.Tensor(7.932319, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0018610427, shape=(), dtype=float32)\n",
      "epoch = 9\n",
      "Gen loss = tf.Tensor(8.087377, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0016379477, shape=(), dtype=float32)\n",
      "epoch = 10\n",
      "Gen loss = tf.Tensor(8.231082, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0014864466, shape=(), dtype=float32)\n",
      "epoch = 11\n",
      "Gen loss = tf.Tensor(8.454046, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.001193021, shape=(), dtype=float32)\n",
      "epoch = 12\n",
      "Gen loss = tf.Tensor(8.435104, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.001196351, shape=(), dtype=float32)\n",
      "epoch = 13\n",
      "Gen loss = tf.Tensor(8.852181, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0010975964, shape=(), dtype=float32)\n",
      "epoch = 14\n",
      "Gen loss = tf.Tensor(8.7898855, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00090198504, shape=(), dtype=float32)\n",
      "epoch = 15\n",
      "Gen loss = tf.Tensor(8.714322, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0006943965, shape=(), dtype=float32)\n",
      "epoch = 16\n",
      "Gen loss = tf.Tensor(8.780369, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0005549059, shape=(), dtype=float32)\n",
      "epoch = 17\n",
      "Gen loss = tf.Tensor(8.834788, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0005261218, shape=(), dtype=float32)\n",
      "epoch = 18\n",
      "Gen loss = tf.Tensor(9.005659, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00047371976, shape=(), dtype=float32)\n",
      "epoch = 19\n",
      "Gen loss = tf.Tensor(8.906237, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0004233293, shape=(), dtype=float32)\n",
      "epoch = 20\n",
      "Gen loss = tf.Tensor(9.060777, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00038604822, shape=(), dtype=float32)\n",
      "epoch = 21\n",
      "Gen loss = tf.Tensor(9.077851, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00036368775, shape=(), dtype=float32)\n",
      "epoch = 22\n",
      "Gen loss = tf.Tensor(9.122993, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00036776974, shape=(), dtype=float32)\n",
      "epoch = 23\n",
      "Gen loss = tf.Tensor(9.21205, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00040679713, shape=(), dtype=float32)\n",
      "epoch = 24\n",
      "Gen loss = tf.Tensor(9.32249, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00034041738, shape=(), dtype=float32)\n",
      "epoch = 25\n",
      "Gen loss = tf.Tensor(9.2149725, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00030616936, shape=(), dtype=float32)\n",
      "epoch = 26\n",
      "Gen loss = tf.Tensor(9.37772, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00024423833, shape=(), dtype=float32)\n",
      "epoch = 27\n",
      "Gen loss = tf.Tensor(9.441716, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00021500513, shape=(), dtype=float32)\n",
      "epoch = 28\n",
      "Gen loss = tf.Tensor(9.502206, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00021047321, shape=(), dtype=float32)\n",
      "epoch = 29\n",
      "Gen loss = tf.Tensor(9.556871, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00022146163, shape=(), dtype=float32)\n",
      "epoch = 30\n",
      "Gen loss = tf.Tensor(9.559451, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00023668991, shape=(), dtype=float32)\n",
      "epoch = 31\n",
      "Gen loss = tf.Tensor(9.543344, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00025143917, shape=(), dtype=float32)\n",
      "epoch = 32\n",
      "Gen loss = tf.Tensor(9.777565, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00020322908, shape=(), dtype=float32)\n",
      "epoch = 33\n",
      "Gen loss = tf.Tensor(9.675578, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00016992765, shape=(), dtype=float32)\n",
      "epoch = 34\n",
      "Gen loss = tf.Tensor(9.795446, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00015802996, shape=(), dtype=float32)\n",
      "epoch = 35\n",
      "Gen loss = tf.Tensor(9.79611, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00015898667, shape=(), dtype=float32)\n",
      "epoch = 36\n",
      "Gen loss = tf.Tensor(9.873155, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00014950892, shape=(), dtype=float32)\n",
      "epoch = 37\n",
      "Gen loss = tf.Tensor(9.8933, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0001425461, shape=(), dtype=float32)\n",
      "epoch = 38\n",
      "Gen loss = tf.Tensor(9.888013, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00016223322, shape=(), dtype=float32)\n",
      "epoch = 39\n",
      "Gen loss = tf.Tensor(9.817178, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0002052205, shape=(), dtype=float32)\n",
      "epoch = 40\n",
      "Gen loss = tf.Tensor(10.048654, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00018746575, shape=(), dtype=float32)\n",
      "epoch = 41\n",
      "Gen loss = tf.Tensor(9.92526, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0001855679, shape=(), dtype=float32)\n",
      "epoch = 42\n",
      "Gen loss = tf.Tensor(9.951167, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00017704452, shape=(), dtype=float32)\n",
      "epoch = 43\n",
      "Gen loss = tf.Tensor(10.171288, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0001450267, shape=(), dtype=float32)\n",
      "epoch = 44\n",
      "Gen loss = tf.Tensor(10.058808, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00011990653, shape=(), dtype=float32)\n",
      "epoch = 45\n",
      "Gen loss = tf.Tensor(10.06839, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00014396098, shape=(), dtype=float32)\n",
      "epoch = 46\n",
      "Gen loss = tf.Tensor(10.124413, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00015164832, shape=(), dtype=float32)\n",
      "epoch = 47\n",
      "Gen loss = tf.Tensor(10.167888, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00012436742, shape=(), dtype=float32)\n",
      "epoch = 48\n",
      "Gen loss = tf.Tensor(10.179704, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00011357712, shape=(), dtype=float32)\n",
      "epoch = 49\n",
      "Gen loss = tf.Tensor(10.187728, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00012451201, shape=(), dtype=float32)\n",
      "epoch = 50\n",
      "Gen loss = tf.Tensor(10.314711, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00014017978, shape=(), dtype=float32)\n",
      "epoch = 51\n",
      "Gen loss = tf.Tensor(10.501407, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00010963783, shape=(), dtype=float32)\n",
      "epoch = 52\n",
      "Gen loss = tf.Tensor(10.335599, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.283443e-05, shape=(), dtype=float32)\n",
      "epoch = 53\n",
      "Gen loss = tf.Tensor(10.43486, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.653466e-05, shape=(), dtype=float32)\n",
      "epoch = 54\n",
      "Gen loss = tf.Tensor(10.396834, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.042877e-05, shape=(), dtype=float32)\n",
      "epoch = 55\n",
      "Gen loss = tf.Tensor(10.428418, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.175743e-05, shape=(), dtype=float32)\n",
      "epoch = 56\n",
      "Gen loss = tf.Tensor(10.428521, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.7790715e-05, shape=(), dtype=float32)\n",
      "epoch = 57\n",
      "Gen loss = tf.Tensor(10.532492, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.8907855e-05, shape=(), dtype=float32)\n",
      "epoch = 58\n",
      "Gen loss = tf.Tensor(10.553206, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.199836e-05, shape=(), dtype=float32)\n",
      "epoch = 59\n",
      "Gen loss = tf.Tensor(10.598237, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.148442e-05, shape=(), dtype=float32)\n",
      "epoch = 60\n",
      "Gen loss = tf.Tensor(10.544458, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.573819e-05, shape=(), dtype=float32)\n",
      "epoch = 61\n",
      "Gen loss = tf.Tensor(10.684093, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.814103e-05, shape=(), dtype=float32)\n",
      "epoch = 62\n",
      "Gen loss = tf.Tensor(10.647662, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.916061e-05, shape=(), dtype=float32)\n",
      "epoch = 63\n",
      "Gen loss = tf.Tensor(10.546372, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.7878994e-05, shape=(), dtype=float32)\n",
      "epoch = 64\n",
      "Gen loss = tf.Tensor(10.614808, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00010078121, shape=(), dtype=float32)\n",
      "epoch = 65\n",
      "Gen loss = tf.Tensor(10.89542, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.0001032888, shape=(), dtype=float32)\n",
      "epoch = 66\n",
      "Gen loss = tf.Tensor(10.814292, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.789792e-05, shape=(), dtype=float32)\n",
      "epoch = 67\n",
      "Gen loss = tf.Tensor(10.712262, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.477268e-05, shape=(), dtype=float32)\n",
      "epoch = 68\n",
      "Gen loss = tf.Tensor(10.795708, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.8327816e-05, shape=(), dtype=float32)\n",
      "epoch = 69\n",
      "Gen loss = tf.Tensor(10.645982, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.6910946e-05, shape=(), dtype=float32)\n",
      "epoch = 70\n",
      "Gen loss = tf.Tensor(11.349349, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.00010528582, shape=(), dtype=float32)\n",
      "epoch = 71\n",
      "Gen loss = tf.Tensor(10.979299, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.6889886e-05, shape=(), dtype=float32)\n",
      "epoch = 72\n",
      "Gen loss = tf.Tensor(11.044024, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(9.120398e-05, shape=(), dtype=float32)\n",
      "epoch = 73\n",
      "Gen loss = tf.Tensor(11.365607, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.126673e-05, shape=(), dtype=float32)\n",
      "epoch = 74\n",
      "Gen loss = tf.Tensor(10.936769, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.8431774e-05, shape=(), dtype=float32)\n",
      "epoch = 75\n",
      "Gen loss = tf.Tensor(11.078022, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(7.5924436e-05, shape=(), dtype=float32)\n",
      "epoch = 76\n",
      "Gen loss = tf.Tensor(11.005839, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(8.300042e-05, shape=(), dtype=float32)\n",
      "epoch = 77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(x_real_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m gen_losses, disc_losses, generated_map \u001b[39m=\u001b[39m train(x_real_train, N_EPOCHS)\n\u001b[1;32m      5\u001b[0m \u001b[39m# denormalise generated map\u001b[39;00m\n\u001b[1;32m      7\u001b[0m generated_map \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m255\u001b[39m\n",
      "Cell \u001b[0;32mIn[210], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, N_EPOCHS)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m map_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     17\u001b[0m     map_batch\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m64\u001b[39m,\u001b[39m64\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     gen_loss, disc_loss, disc_accuracy \u001b[39m=\u001b[39m training_step(map_batch)\n\u001b[1;32m     22\u001b[0m     gen_losses_for_epoch\u001b[39m.\u001b[39mappend(gen_loss)\n\u001b[1;32m     23\u001b[0m     disc_losses_for_epoch\u001b[39m.\u001b[39mappend(disc_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses, generated_map = train(x_real_train, N_EPOCHS)\n",
    "\n",
    "# denormalise generated map\n",
    "\n",
    "generated_map *= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generated_map = generated_map.numpy()\n",
    "\n",
    "generated_map = np.round(generated_map,0)\n",
    "\n",
    "generated_map.reshape(64,64)\n",
    "\n",
    "generated_map = generated_map.tolist()\n",
    "\n",
    "print(generated_map)\n",
    "\n",
    "#write generated map to csv\n",
    "\n",
    "\n",
    "\n",
    "with open('generated_map.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for i in range(64):\n",
    "        writer.writerow(generated_map[0][i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (600,) and (1, 600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epoch_list \u001b[39m=\u001b[39m [\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,N_EPOCHS)]\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39;49mplot(disc_losses, epoch_list, \u001b[39m'\u001b[39;49m\u001b[39mr-\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mplot(gen_losses, epoch_list, \u001b[39m'\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mGen and Disc loss over epochs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (600,) and (1, 600)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = [range(0,N_EPOCHS)]\n",
    "\n",
    "plt.plot(disc_losses, epoch_list, 'r-')\n",
    "plt.plot(gen_losses, epoch_list, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
