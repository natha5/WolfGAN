{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 13:24:54.712105: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 13:24:57.398851: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 13:24:57.410296: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 13:25:02.940765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(3,2.5,size=(1,100))\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Reshape((8,8,60)))\n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(8,8,60)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(16,16,60)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, kernel_size=5, padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3840)              387840    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 1)         61        \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 1)         2         \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 1)         2         \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387905 (1.48 MB)\n",
      "Trainable params: 387905 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "generated_map = gen_model(noise, training = False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 128)       3328      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 131073    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134401 (525.00 KB)\n",
      "Trainable params: 134401 (525.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[0.00584043]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(generated_map)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(np.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(np.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(np.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(1e-4)\n",
    "disc_optimizer = Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 48\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch, gen_losses, disc_losses):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        fake_output = disc_model(generated_map, training = True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        gen_losses.append(gen_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            print(map_batch.shape)\n",
    "            gen_loss, disc_loss = training_step(map_batch)\n",
    "\n",
    "            gen_losses.append(gen_loss)\n",
    "            disc_losses.append(disc_loss)\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    map_after_training = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    print(map_after_training)\n",
    "\n",
    "    return gen_losses, disc_losses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 64, 64, 1)\n",
      "(64, 64, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function `training_step` failed due to `missing a required argument: 'gen_losses'`. Received args: (array([[[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       ...,\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]]]),) and kwargs: {} for signature: (current_batch, gen_losses, disc_losses).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(x_real_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m gen_losses, disc_losses \u001b[39m=\u001b[39m train(x_real_train, N_EPOCHS)\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, N_EPOCHS)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m map_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(map_batch\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m     gen_loss, disc_loss \u001b[39m=\u001b[39m training_step(map_batch)\n\u001b[1;32m     13\u001b[0m     gen_losses\u001b[39m.\u001b[39mappend(gen_loss)\n\u001b[1;32m     14\u001b[0m     disc_losses\u001b[39m.\u001b[39mappend(disc_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py:406\u001b[0m, in \u001b[0;36mFunctionSpec.bind_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m   bound_arguments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind_with_defaults(\n\u001b[1;32m    404\u001b[0m       args, sanitized_kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_values)\n\u001b[1;32m    405\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 406\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBinding inputs to tf.function `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name\u001b[39m}\u001b[39;00m\u001b[39m` failed due to `\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived args: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m and kwargs: \u001b[39m\u001b[39m{\u001b[39;00msanitized_kwargs\u001b[39m}\u001b[39;00m\u001b[39m for signature:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m   ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mreturn\u001b[39;00m bound_arguments\u001b[39m.\u001b[39margs, bound_arguments\u001b[39m.\u001b[39mkwargs\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function `training_step` failed due to `missing a required argument: 'gen_losses'`. Received args: (array([[[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       ...,\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]],\n\n       [[18],\n        [18],\n        [18],\n        ...,\n        [35],\n        [35],\n        [35]]]),) and kwargs: {} for signature: (current_batch, gen_losses, disc_losses)."
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses = train(x_real_train, N_EPOCHS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disc_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epoch_list \u001b[39m=\u001b[39m [\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,N_EPOCHS)]\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(disc_losses, epoch_list, \u001b[39m'\u001b[39m\u001b[39mr-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(gen_losses, epoch_list, \u001b[39m'\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mGen and Disc loss over epochs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'disc_losses' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_list = [range(0,N_EPOCHS)]\n",
    "plt.plot(disc_losses, epoch_list, 'r-')\n",
    "plt.plot(gen_losses, epoch_list, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
