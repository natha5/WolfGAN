{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 09:42:05.648184: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-11 09:42:05.704344: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-11 09:42:05.705880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 09:42:06.975732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 1,64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize dataset data into range of tanh (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real_train = x_real_train.astype('float32')\n",
    "x_real_train /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(50,2,size=(1,100))\n",
    "    input = input * 10\n",
    "    \n",
    "\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8,8,60)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(32, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(8,8,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(16, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(16,16,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (5,5), strides=(2,2), padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "   \n",
    "    model.add(Conv2D(1024, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(2048, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3840)              387840    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 3840)              15360     \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 128)       7680      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 64)        8192      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 1)         64        \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 419904 (1.60 MB)\n",
      "Trainable params: 411840 (1.57 MB)\n",
      "Non-trainable params: 8064 (31.50 KB)\n",
      "_________________________________________________________________\n",
      "tf.Tensor(\n",
      "[[[[-1.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]\n",
      "\n",
      "  [[ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]\n",
      "\n",
      "  [[ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]\n",
      "\n",
      "  [[ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]\n",
      "\n",
      "  [[ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   ...\n",
      "   [ 0.]\n",
      "   [ 0.]\n",
      "   [ 0.]]]], shape=(1, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "test_gen = gen_model(noise, training = False)\n",
    "\n",
    "print(test_gen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 09:42:12.443254: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104857600 exceeds 10% of free system memory.\n",
      "2023-08-11 09:42:12.611001: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104857600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 256)       6656      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 256)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 512)         3277312   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 2048)        26216448  \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 2, 2, 2048)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 2048)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 1, 2048)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29502465 (112.54 MB)\n",
      "Trainable params: 29502465 (112.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[-0.00697251]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 09:42:12.666818: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104857600 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(test_gen)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(learning_rate =1e-4, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate =1e-4, beta_1=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrim_accuracy(real_output, fake_output):\n",
    "    if real_output >=0:\n",
    "        \n",
    "        if fake_output <0:\n",
    "            accuracy = (real_output + fake_output) / (real_output + fake_output)\n",
    "        else:\n",
    "            accuracy = real_output/ (real_output + fake_output)\n",
    "    elif fake_output <0:\n",
    "        accuracy = fake_output / (real_output + fake_output)\n",
    "    else:\n",
    "        accuracy = 0/ (real_output + fake_output)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 25\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "        print(\"generated map shape\" + str(generated_map.shape))\n",
    "        \n",
    "        fake_output = disc_model(generated_map,  training=True)\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        \n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        disc_accuracy = compute_discrim_accuracy(real_output, fake_output)\n",
    "\n",
    "        \n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, disc_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    disc_accuracies = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        gen_losses_for_epoch = []\n",
    "        disc_losses_for_epoch = []\n",
    "\n",
    "        print(\"epoch = \" + str(epoch))\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            \n",
    "            map_batch.reshape(1,64,64,1)\n",
    "\n",
    "            \n",
    "            gen_loss, disc_loss, disc_accuracy = training_step(map_batch)\n",
    "\n",
    "            gen_losses_for_epoch.append(gen_loss)\n",
    "            disc_losses_for_epoch.append(disc_loss)\n",
    "            disc_accuracies.append(disc_accuracy)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses_for_epoch) / 48\n",
    "        avg_disc_loss = sum(disc_losses_for_epoch) / 48\n",
    "\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "\n",
    "        print(\"Gen loss = \" + str(avg_gen_loss))\n",
    "        print(\"Disc loss = \" + str(avg_disc_loss))\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    generated_map = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    \n",
    "\n",
    "    return gen_losses, disc_losses, generated_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 64, 64, 1)\n",
      "epoch = 0\n",
      "generated map shape(1, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 09:42:19.278309: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104857600 exceeds 10% of free system memory.\n",
      "2023-08-11 09:42:19.321890: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 104857600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated map shape(1, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 09:42:22.204763: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_BOOL\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond/PartitionedCall/cond_3/output/_20'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen loss = tf.Tensor(1.1123257, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.77162427, shape=(), dtype=float32)\n",
      "epoch = 1\n",
      "Gen loss = tf.Tensor(1.3132476, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265294, shape=(), dtype=float32)\n",
      "epoch = 2\n",
      "Gen loss = tf.Tensor(1.3132491, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62652916, shape=(), dtype=float32)\n",
      "epoch = 3\n",
      "Gen loss = tf.Tensor(1.3132486, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265287, shape=(), dtype=float32)\n",
      "epoch = 4\n",
      "Gen loss = tf.Tensor(1.3132463, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265301, shape=(), dtype=float32)\n",
      "epoch = 5\n",
      "Gen loss = tf.Tensor(1.3132439, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653047, shape=(), dtype=float32)\n",
      "epoch = 6\n",
      "Gen loss = tf.Tensor(1.3132347, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653404, shape=(), dtype=float32)\n",
      "epoch = 7\n",
      "Gen loss = tf.Tensor(1.3132387, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265328, shape=(), dtype=float32)\n",
      "epoch = 8\n",
      "Gen loss = tf.Tensor(1.3132316, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265355, shape=(), dtype=float32)\n",
      "epoch = 9\n",
      "Gen loss = tf.Tensor(1.3132273, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265367, shape=(), dtype=float32)\n",
      "epoch = 10\n",
      "Gen loss = tf.Tensor(1.313217, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265411, shape=(), dtype=float32)\n",
      "epoch = 11\n",
      "Gen loss = tf.Tensor(1.3132215, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265388, shape=(), dtype=float32)\n",
      "epoch = 12\n",
      "Gen loss = tf.Tensor(1.3132136, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654185, shape=(), dtype=float32)\n",
      "epoch = 13\n",
      "Gen loss = tf.Tensor(1.3132104, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265428, shape=(), dtype=float32)\n",
      "epoch = 14\n",
      "Gen loss = tf.Tensor(1.3132176, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654024, shape=(), dtype=float32)\n",
      "epoch = 15\n",
      "Gen loss = tf.Tensor(1.313219, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265409, shape=(), dtype=float32)\n",
      "epoch = 16\n",
      "Gen loss = tf.Tensor(1.3132151, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654155, shape=(), dtype=float32)\n",
      "epoch = 17\n",
      "Gen loss = tf.Tensor(1.3132075, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265447, shape=(), dtype=float32)\n",
      "epoch = 18\n",
      "Gen loss = tf.Tensor(1.3132144, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265417, shape=(), dtype=float32)\n",
      "epoch = 19\n",
      "Gen loss = tf.Tensor(1.3132222, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265391, shape=(), dtype=float32)\n",
      "epoch = 20\n",
      "Gen loss = tf.Tensor(1.3132218, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265387, shape=(), dtype=float32)\n",
      "epoch = 21\n",
      "Gen loss = tf.Tensor(1.3132194, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654084, shape=(), dtype=float32)\n",
      "epoch = 22\n",
      "Gen loss = tf.Tensor(1.3132309, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653595, shape=(), dtype=float32)\n",
      "epoch = 23\n",
      "Gen loss = tf.Tensor(1.3132285, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653714, shape=(), dtype=float32)\n",
      "epoch = 24\n",
      "Gen loss = tf.Tensor(1.313234, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265344, shape=(), dtype=float32)\n",
      "(64, 64)\n",
      "[[-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, -255.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0, 10.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0, 11.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses, generated_map = train(x_real_train, N_EPOCHS)\n",
    "\n",
    "# denormalise generated map\n",
    "\n",
    "generated_map *= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generated_map = generated_map.numpy()\n",
    "\n",
    "generated_map = np.round(generated_map,0)\n",
    "\n",
    "generated_map = np.reshape(generated_map, (64,64))\n",
    "\n",
    "print(generated_map.shape)\n",
    "\n",
    "#write generated map to csv\n",
    "\n",
    "\n",
    "np.savetxt('generated_map.csv', generated_map, delimiter=',')\n",
    "\n",
    "generated_map = generated_map.tolist()\n",
    "\n",
    "print(generated_map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = [i for i in range(0,N_EPOCHS)]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epoch_list,disc_losses,  'r-')\n",
    "plt.plot(epoch_list, gen_losses, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
