{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Conv2D,Conv2DTranspose, Dropout, Reshape, MaxPooling2D, Flatten, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', header=None)\n",
    "\n",
    "\n",
    "df = df.values.reshape(60, 1,64, 64, 1)\n",
    "\n",
    "labels = np.zeros(60)\n",
    "\n",
    "x_real_train, x_real_test = train_test_split(df, test_size=0.2) #12 test values\n",
    "y_real_train, y_real_test = train_test_split(labels, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize dataset data into range of tanh (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real_train = x_real_train.astype('float32')\n",
    "x_real_train /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create points in latent space to be fed into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_input():\n",
    "    input = np.random.normal(50,2,size=(1,100))\n",
    "    input = input * 10\n",
    "    \n",
    "\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(60*8*8, input_shape=(100,)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8,8,60)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(8,8,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "  \n",
    "    \n",
    "    model.add(Conv2DTranspose(64, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(16,16,60)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (1,1), strides=(2,2), padding='same', use_bias=False, input_shape=(32,32,60)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "              \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_discriminator():\n",
    "    \n",
    "    # 1st set of layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (5,5), strides=(2,2), padding=\"same\", input_shape=(64,64,1)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "   \n",
    "    model.add(Conv2D(2048, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    \n",
    "    # output layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1)) # Binary classification (2 outputs), so only 1 dense layer needed\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the models from the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 3840)              387840    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 3840)              15360     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 3840)              0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 8, 8, 60)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2D  (None, 16, 16, 128)       7808      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 16, 16, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2D  (None, 32, 32, 64)        8256      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 32, 32, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2D  (None, 64, 64, 1)         65        \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420097 (1.60 MB)\n",
      "Trainable params: 412033 (1.57 MB)\n",
      "Non-trainable params: 8064 (31.50 KB)\n",
      "_________________________________________________________________\n",
      "tf.Tensor(\n",
      "[[[[1.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]], shape=(1, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gen_model = make_generator()\n",
    "\n",
    "noise = generate_generator_input()\n",
    "test_gen = gen_model(noise, training = False)\n",
    "\n",
    "print(test_gen)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test the untrained discriminator on the map of noise generated before\n",
    "\n",
    "Negative values means fake, positive means real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator()\n",
    "decision = disc_model(test_gen)\n",
    "print(decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator loss, adapted from: https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = Adam(1e-4)\n",
    "disc_optimizer = Adam(1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrim_accuracy(real_output, fake_output):\n",
    "    if real_output >=0:\n",
    "        \n",
    "        if fake_output <0:\n",
    "            accuracy = (real_output + fake_output) / (real_output + fake_output)\n",
    "        else:\n",
    "            accuracy = real_output/ (real_output + fake_output)\n",
    "    elif fake_output <0:\n",
    "        accuracy = fake_output / (real_output + fake_output)\n",
    "    else:\n",
    "        accuracy = 0/ (real_output + fake_output)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 25\n",
    "\n",
    "VERBOSE = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(current_batch):\n",
    "    \n",
    "    noise_sample = generate_generator_input()\n",
    "    \n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_map = gen_model(noise_sample, training=True)\n",
    "        print(\"generated map shape\" + str(generated_map.shape))\n",
    "        \n",
    "        fake_output = disc_model(generated_map,  training=True)\n",
    "        real_output = disc_model(current_batch, training=True)\n",
    "        \n",
    "\n",
    "        gen_loss = generator_loss(fake_output=fake_output)\n",
    "        disc_loss = discrim_loss(real_output=real_output, fake_output=fake_output)\n",
    "\n",
    "        disc_accuracy = compute_discrim_accuracy(real_output, fake_output)\n",
    "\n",
    "        \n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, gen_model.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, disc_model.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, disc_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, N_EPOCHS):\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    disc_accuracies = []\n",
    "    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        gen_losses_for_epoch = []\n",
    "        disc_losses_for_epoch = []\n",
    "\n",
    "        print(\"epoch = \" + str(epoch))\n",
    "\n",
    "        for map_batch in dataset:\n",
    "            \n",
    "            map_batch.reshape(1,64,64,1)\n",
    "\n",
    "            \n",
    "            gen_loss, disc_loss, disc_accuracy = training_step(map_batch)\n",
    "\n",
    "            gen_losses_for_epoch.append(gen_loss)\n",
    "            disc_losses_for_epoch.append(disc_loss)\n",
    "            disc_accuracies.append(disc_accuracy)\n",
    "        \n",
    "        avg_gen_loss = sum(gen_losses_for_epoch) / 48\n",
    "        avg_disc_loss = sum(disc_losses_for_epoch) / 48\n",
    "\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "\n",
    "        print(\"Gen loss = \" + str(avg_gen_loss))\n",
    "        print(\"Disc loss = \" + str(avg_disc_loss))\n",
    "    \n",
    "    input_for_map_after_training = generate_generator_input()\n",
    "    generated_map = gen_model(input_for_map_after_training, training=False)\n",
    "\n",
    "    \n",
    "\n",
    "    return gen_losses, disc_losses, generated_map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 64, 64, 1)\n",
      "epoch = 0\n",
      "generated map shape(1, 64, 64, 1)\n",
      "generated map shape(1, 64, 64, 1)\n",
      "Gen loss = tf.Tensor(1.0695251, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.8088169, shape=(), dtype=float32)\n",
      "epoch = 1\n",
      "Gen loss = tf.Tensor(1.3128768, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62668675, shape=(), dtype=float32)\n",
      "epoch = 2\n",
      "Gen loss = tf.Tensor(1.3128879, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266809, shape=(), dtype=float32)\n",
      "epoch = 3\n",
      "Gen loss = tf.Tensor(1.3129724, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266429, shape=(), dtype=float32)\n",
      "epoch = 4\n",
      "Gen loss = tf.Tensor(1.3130162, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266333, shape=(), dtype=float32)\n",
      "epoch = 5\n",
      "Gen loss = tf.Tensor(1.3130647, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62661666, shape=(), dtype=float32)\n",
      "epoch = 6\n",
      "Gen loss = tf.Tensor(1.3131012, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6266024, shape=(), dtype=float32)\n",
      "epoch = 7\n",
      "Gen loss = tf.Tensor(1.3131205, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.626579, shape=(), dtype=float32)\n",
      "epoch = 8\n",
      "Gen loss = tf.Tensor(1.3131374, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62657505, shape=(), dtype=float32)\n",
      "epoch = 9\n",
      "Gen loss = tf.Tensor(1.3131605, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265679, shape=(), dtype=float32)\n",
      "epoch = 10\n",
      "Gen loss = tf.Tensor(1.3131799, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62655735, shape=(), dtype=float32)\n",
      "epoch = 11\n",
      "Gen loss = tf.Tensor(1.313202, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265495, shape=(), dtype=float32)\n",
      "epoch = 12\n",
      "Gen loss = tf.Tensor(1.3131897, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265529, shape=(), dtype=float32)\n",
      "epoch = 13\n",
      "Gen loss = tf.Tensor(1.3132056, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654966, shape=(), dtype=float32)\n",
      "epoch = 14\n",
      "Gen loss = tf.Tensor(1.3131951, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62655133, shape=(), dtype=float32)\n",
      "epoch = 15\n",
      "Gen loss = tf.Tensor(1.3131968, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62655085, shape=(), dtype=float32)\n",
      "epoch = 16\n",
      "Gen loss = tf.Tensor(1.3132066, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265479, shape=(), dtype=float32)\n",
      "epoch = 17\n",
      "Gen loss = tf.Tensor(1.3131987, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62655085, shape=(), dtype=float32)\n",
      "epoch = 18\n",
      "Gen loss = tf.Tensor(1.3132125, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654376, shape=(), dtype=float32)\n",
      "epoch = 19\n",
      "Gen loss = tf.Tensor(1.3132082, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62654847, shape=(), dtype=float32)\n",
      "epoch = 20\n",
      "Gen loss = tf.Tensor(1.3132106, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265454, shape=(), dtype=float32)\n",
      "epoch = 21\n",
      "Gen loss = tf.Tensor(1.3132184, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265426, shape=(), dtype=float32)\n",
      "epoch = 22\n",
      "Gen loss = tf.Tensor(1.3132275, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.6265383, shape=(), dtype=float32)\n",
      "epoch = 23\n",
      "Gen loss = tf.Tensor(1.3132349, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653786, shape=(), dtype=float32)\n",
      "epoch = 24\n",
      "Gen loss = tf.Tensor(1.313233, shape=(), dtype=float32)\n",
      "Disc loss = tf.Tensor(0.62653565, shape=(), dtype=float32)\n",
      "(1, 64, 64, 1)\n",
      "[[[[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [-255.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]], [[10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0], [10.0], [2.0]], [[2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0], [2.0]]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 4D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(generated_map)\n\u001b[1;32m     24\u001b[0m \u001b[39m#write generated map to csv\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m np\u001b[39m.\u001b[39;49msavetxt(\u001b[39m'\u001b[39;49m\u001b[39mgenerated_map.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, generated_map, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:1555\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[39m# Handle 1-dimensional arrays\u001b[39;00m\n\u001b[1;32m   1554\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1556\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected 1D or 2D array, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mD array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m X\u001b[39m.\u001b[39mndim)\n\u001b[1;32m   1557\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1558\u001b[0m     \u001b[39m# Common case -- 1d array of numbers\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mnames \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 4D array instead"
     ]
    }
   ],
   "source": [
    "print(x_real_train.shape)\n",
    "\n",
    "gen_losses, disc_losses, generated_map = train(x_real_train, N_EPOCHS)\n",
    "\n",
    "# denormalise generated map\n",
    "\n",
    "generated_map *= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generated_map = generated_map.numpy()\n",
    "\n",
    "generated_map = np.round(generated_map,0)\n",
    "\n",
    "generated_map = np.reshape(generated_map, (64,64))\n",
    "\n",
    "print(generated_map.shape)\n",
    "\n",
    "#write generated map to csv\n",
    "\n",
    "\n",
    "np.savetxt('generated_map.csv', generated_map, delimiter=',')\n",
    "\n",
    "generated_map = generated_map.tolist()\n",
    "\n",
    "print(generated_map)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = [i for i in range(0,N_EPOCHS)]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epoch_list,disc_losses,  'r-')\n",
    "plt.plot(epoch_list, gen_losses, 'b-')\n",
    "plt.title('Gen and Disc loss over epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
